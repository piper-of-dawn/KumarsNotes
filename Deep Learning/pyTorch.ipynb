{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- PyTorch uses tensors as a basic datatype. Tensors are generalizations of matrices to an arbitrary number of dimensions (note that in the context of tensors, a dimension is often called an axis).\n",
    "- Imagine a colored pixel. It has three dimensions: X, Y, and color."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is CUDA?\n",
    "CUDA stands for Compute Unified Device Architecture. It is a parallel computing platform and application programming interface (API) model created by Nvidia. It allows software developers and software engineers to use a CUDA-enabled graphics processing unit (GPU) for general purpose processing â€“ an approach termed GPGPU (General-Purpose computing on Graphics Processing Units). The CUDA platform is a software layer that gives direct access to the GPU's virtual instruction set and parallel computational elements, for the execution of compute kernels.\n",
    "CUDA acts as a compute environment for PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7983, 0.0416, 0.6330],\n",
       "        [0.9983, 0.3587, 0.4066],\n",
       "        [0.1612, 0.3614, 0.8665],\n",
       "        [0.2395, 0.2811, 0.1859],\n",
       "        [0.6195, 0.9561, 0.3588]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.rand(5, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[3., 2.],\n",
       "         [1., 5.]],\n",
       "\n",
       "        [[7., 9.],\n",
       "         [2., 1.]],\n",
       "\n",
       "        [[3., 4.],\n",
       "         [5., 6.]]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a tensor of size (5 x 3)\n",
    "tensor = torch.Tensor([[[3,2],[1,5]], [[7,9], [2,1]], [[3,4], [5,6]]])\n",
    "tensor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View the device of the tensor\n",
    "tensor.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[3., 2.],\n",
       "         [1., 5.]],\n",
       "\n",
       "        [[7., 9.],\n",
       "         [2., 1.]],\n",
       "\n",
       "        [[3., 4.],\n",
       "         [5., 6.]]], device='cuda:0')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Change the device of the tensor from CPU to GPU\n",
    "tensor.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2, 2])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the shape of the tensor\n",
    "tensor.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_It tells us that we have 3 elements in the first dimension, 2 elements in the second dimension, and 2 elements in the third dimension. The total number of elements is 3 x 2 x 2 = 8._"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Element Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first matrix \n",
      " tensor([[3., 2.],\n",
      "        [1., 5.]])\n",
      "The first matrix inside the first matrix \n",
      " tensor([3., 2.])\n"
     ]
    }
   ],
   "source": [
    "print(f\"The first matrix \\n {tensor[0]}\") # Gives the first matrix\n",
    "print(f\"The first matrix inside the first matrix \\n {tensor[0][0]}\") "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "linear = nn.Linear(10,2) # Creates a linear layer with 10 inputs and 2 outputs. It takes in a tensor of size (N, 10) and outputs a tensor of size (N, 2).\n",
    "input = torch.randn(3,10) # Creates a random tensor of size (3, 10)\n",
    "output = linear(input) # Passes the input tensor through the linear layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.2551,  0.1872],\n",
      "        [ 1.0104, -0.3893],\n",
      "        [ 0.4198, -0.5051]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is an activation?\n",
    "Activation takes any number and outputs a number between 0 and 1. It is a function that is applied to the output of each processing element (neuron) in a neural network.\n",
    "\n",
    "## ReLU\n",
    "ReLu stands for Rectified Linear Unit. It is a type of activation function. Mathematically, it is defined as y = max(0, x). It is a simple function which returns the value passed to it directly, or the value 0, whichever is greater. It has become very popular in the last few years and is now the default activation function for many types of neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "relu = nn.ReLU() # Creates a ReLU activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0000, 0.1872],\n",
      "        [1.0104, 0.0000],\n",
      "        [0.4198, 0.0000]], grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "relu_output = relu(output) # Passes the output tensor through the ReLU activation function\n",
    "print(relu_output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What are optimisers?\n",
    "Optimisers are the algorithms that are used to calculate the loss and adjust the weights. They are used to solve minimisation problems (minimising the loss). They help us finetune our neural network to get the best possible results. We need `torch.optim` for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can sequentially peroform a series of steps using nn.sequential."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = nn.Sequential(\n",
    "  nn.Linear(5,2), # Creates a linear layer with 5 inputs and 2 outputs. This is linear transformation.\n",
    "  nn.BatchNorm1d(2), # Creates a batch normalization layer with 2 features. This is normalization.\n",
    "  nn.ReLU(), # Creates a ReLU activation function\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.2417, 0.6178],\n",
      "        [0.0000, 0.7928],\n",
      "        [0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "input = torch.randn(3,5) # Creates a random tensor of size (3, 5) \n",
    "mlp_output = mlp(input) # Passes the input tensor through the MLP\n",
    "print(mlp_output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will use a optimiser. We will use the Adam optimiser. It is a very popular optimiser. It is a combination of RMSProp and Stochastic Gradient Descent with momentum. It is an adaptive learning rate optimiser.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam_optimiser = optim.Adam(mlp.parameters(), lr=0.01) # Creates an Adam optimizer with a learning rate of 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(78.4811, grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "train_data = torch.randn(100,5) # Creates a random tensor of size (100, 5)\n",
    "adam_optimiser.zero_grad() # Clears the gradients of the optimizer\n",
    "current_loss = torch.abs(mlp(train_data)).sum() # Calculates the loss of the MLP on the training data \n",
    "current_loss.backward() # Backpropagates the loss\n",
    "adam_optimiser.step() # Updates the parameters of the MLP\n",
    "print(current_loss)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is one single step of training. We will use a loop to train for multiple epochs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "41cec28fd681075328d5c3ce57c3e36d2e28950cd4b5df3e126fcfc4cca87be8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
